slurmstepd: info: Setting TMPDIR to /scratch/13439844. Previous errors about TMPDIR can be discarded
2025-06-12 23:20:49,318 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.208.6:41143'
2025-06-12 23:21:09,427 - distributed.worker - INFO -       Start worker at:   tcp://10.120.208.6:46177
2025-06-12 23:21:09,427 - distributed.worker - INFO -          Listening to:   tcp://10.120.208.6:46177
2025-06-12 23:21:09,428 - distributed.worker - INFO -           Worker name:             SLURMCluster-2
2025-06-12 23:21:09,428 - distributed.worker - INFO -          dashboard at:         10.120.208.6:44363
2025-06-12 23:21:09,428 - distributed.worker - INFO - Waiting to connect to:   tcp://10.120.209.3:36575
2025-06-12 23:21:09,428 - distributed.worker - INFO - -------------------------------------------------
2025-06-12 23:21:09,428 - distributed.worker - INFO -               Threads:                         32
2025-06-12 23:21:09,428 - distributed.worker - INFO -                Memory:                  37.25 GiB
2025-06-12 23:21:09,428 - distributed.worker - INFO -       Local Directory: /scratch/13439844/dask-scratch-space/worker-6c1w1buc
2025-06-12 23:21:09,428 - distributed.worker - INFO - -------------------------------------------------
2025-06-12 23:21:09,552 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-06-12 23:21:09,552 - distributed.worker - INFO -         Registered to:   tcp://10.120.209.3:36575
2025-06-12 23:21:09,552 - distributed.worker - INFO - -------------------------------------------------
2025-06-12 23:21:09,552 - distributed.core - INFO - Starting established connection to tcp://10.120.209.3:36575
2025-06-12 23:23:32,333 - distributed.core - INFO - Event loop was unresponsive in Worker for 45.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[23:24:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

2025-06-12 23:24:25,049 - distributed.sizeof - WARNING - ("Sizeof calculation for object of type 'dict' failed. Defaulting to -1 B",)
slurmstepd: error: *** JOB 13439844 ON a100-22 CANCELLED AT 2025-06-12T23:24:31 ***
2025-06-12 23:24:31,242 - distributed.worker - INFO - Stopping worker at tcp://10.120.208.6:46177. Reason: scheduler-close
