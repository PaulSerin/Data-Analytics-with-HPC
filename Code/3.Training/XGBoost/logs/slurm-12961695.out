slurmstepd: info: Setting TMPDIR to /scratch/12961695. Previous errors about TMPDIR can be discarded
2025-05-26 18:58:49,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.209.8:43467'
2025-05-26 18:58:55,251 - distributed.worker - INFO -       Start worker at:   tcp://10.120.209.8:45471
2025-05-26 18:58:55,252 - distributed.worker - INFO -          Listening to:   tcp://10.120.209.8:45471
2025-05-26 18:58:55,252 - distributed.worker - INFO -           Worker name:             SLURMCluster-3
2025-05-26 18:58:55,252 - distributed.worker - INFO -          dashboard at:         10.120.209.8:34041
2025-05-26 18:58:55,252 - distributed.worker - INFO - Waiting to connect to:  tcp://10.120.207.12:32817
2025-05-26 18:58:55,252 - distributed.worker - INFO - -------------------------------------------------
2025-05-26 18:58:55,252 - distributed.worker - INFO -               Threads:                         32
2025-05-26 18:58:55,252 - distributed.worker - INFO -                Memory:                  37.25 GiB
2025-05-26 18:58:55,252 - distributed.worker - INFO -       Local Directory: /scratch/12961695/dask-scratch-space/worker-snj0gr_z
2025-05-26 18:58:55,252 - distributed.worker - INFO - -------------------------------------------------
2025-05-26 18:58:58,087 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-05-26 18:58:58,087 - distributed.worker - INFO -         Registered to:  tcp://10.120.207.12:32817
2025-05-26 18:58:58,087 - distributed.worker - INFO - -------------------------------------------------
2025-05-26 18:58:58,087 - distributed.core - INFO - Starting established connection to tcp://10.120.207.12:32817
2025-05-26 19:00:45,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[19:01:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

2025-05-26 19:01:47,506 - distributed.sizeof - WARNING - ("Sizeof calculation for object of type 'dict' failed. Defaulting to -1 B",)
2025-05-26 19:01:51,374 - distributed.worker - INFO - Stopping worker at tcp://10.120.209.8:45471. Reason: scheduler-close
slurmstepd: error: *** JOB 12961695 ON a100-40 CANCELLED AT 2025-05-26T19:01:51 ***
