slurmstepd: info: Setting TMPDIR to /scratch/12961696. Previous errors about TMPDIR can be discarded
2025-05-26 18:58:49,865 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.120.209.12:41701'
2025-05-26 18:58:55,295 - distributed.worker - INFO -       Start worker at:  tcp://10.120.209.12:33603
2025-05-26 18:58:55,295 - distributed.worker - INFO -          Listening to:  tcp://10.120.209.12:33603
2025-05-26 18:58:55,295 - distributed.worker - INFO -           Worker name:             SLURMCluster-1
2025-05-26 18:58:55,295 - distributed.worker - INFO -          dashboard at:        10.120.209.12:43779
2025-05-26 18:58:55,295 - distributed.worker - INFO - Waiting to connect to:  tcp://10.120.207.12:32817
2025-05-26 18:58:55,295 - distributed.worker - INFO - -------------------------------------------------
2025-05-26 18:58:55,295 - distributed.worker - INFO -               Threads:                         32
2025-05-26 18:58:55,295 - distributed.worker - INFO -                Memory:                  37.25 GiB
2025-05-26 18:58:55,295 - distributed.worker - INFO -       Local Directory: /scratch/12961696/dask-scratch-space/worker-z2z_zzx0
2025-05-26 18:58:55,295 - distributed.worker - INFO - -------------------------------------------------
2025-05-26 18:58:58,061 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-05-26 18:58:58,061 - distributed.worker - INFO -         Registered to:  tcp://10.120.207.12:32817
2025-05-26 18:58:58,061 - distributed.worker - INFO - -------------------------------------------------
2025-05-26 18:58:58,061 - distributed.core - INFO - Starting established connection to tcp://10.120.207.12:32817
2025-05-26 18:59:13,808 - distributed.core - INFO - Event loop was unresponsive in Worker for 15.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
[19:01:11] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.
Potential solutions:
- Use a data structure that matches the device ordinal in the booster.
- Set the device for booster before call to inplace_predict.

This warning will only be shown once.

2025-05-26 19:01:47,503 - distributed.sizeof - WARNING - ("Sizeof calculation for object of type 'dict' failed. Defaulting to -1 B",)
slurmstepd: error: *** JOB 12961696 ON a100-44 CANCELLED AT 2025-05-26T19:01:51 ***
2025-05-26 19:01:51,374 - distributed.worker - INFO - Stopping worker at tcp://10.120.209.12:33603. Reason: scheduler-close