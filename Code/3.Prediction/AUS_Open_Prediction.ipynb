{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4319b6",
   "metadata": {},
   "source": [
    "## AUSTRALIAN OPEN 2025 - PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da1489de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from importlib.machinery import SourceFileLoader\n",
    "\n",
    "utils_dir = Path.cwd() / \"Code\" / \"0.Utils\"\n",
    "sys.path.insert(0, str(utils_dir))\n",
    "import utils\n",
    "from utils import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ca437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Répertoire courant (cwd) : /mnt/netapp2/Store_uni/home/ulc/cursos/curso363/TFM/Data-Analytics-with-HPC\n",
      "→ Contenu du répertoire courant :\n",
      "    .git\n",
      "    .DS_Store\n",
      "    .gitignore\n",
      "    Code\n",
      "    Data\n",
      "    Images\n",
      "    README.md\n",
      "    Dictionnary.md\n",
      "    requirements.txt\n",
      "    requirements_pip.txt\n",
      "    Models\n",
      "    RAF.txt\n",
      "    Datasets\n",
      "\n",
      "→ Contenu du dossier parent :\n",
      "    .git\n",
      "    .DS_Store\n",
      "    .gitignore\n",
      "    Code\n",
      "    Data\n",
      "    Images\n",
      "    README.md\n",
      "    Dictionnary.md\n",
      "    requirements.txt\n",
      "    requirements_pip.txt\n",
      "    Models\n",
      "    RAF.txt\n",
      "    Datasets\n"
     ]
    }
   ],
   "source": [
    "# 1) Debug : où suis-je et quel contenu ?\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"→ Répertoire courant (cwd) :\", os.getcwd())\n",
    "print(\"→ Contenu du répertoire courant :\")\n",
    "for p in Path('.').iterdir():\n",
    "    print(\"   \", p)\n",
    "\n",
    "# 2) Si tu ne vois pas Datasets là, regardons un niveau au-dessus\n",
    "parent = Path('.').parent\n",
    "print(\"\\n→ Contenu du dossier parent :\")\n",
    "for p in parent.iterdir():\n",
    "    print(\"   \", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab81f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# Vérifie que la commande nvidia-smi est disponible\n",
    "if shutil.which(\"nvidia-smi\") is None:\n",
    "    print(\"nvidia-smi non trouvé : pas de GPU NVIDIA détectée ou pilote non installé.\")\n",
    "else:\n",
    "    try:\n",
    "        # Récupère la liste des GPU\n",
    "        output = subprocess.check_output(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name\", \"--format=csv,noheader\"], \n",
    "            stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        gpus = output.decode().strip().split(\"\\n\")\n",
    "        print(f\"{len(gpus)} GPU(s) détectée(s) :\")\n",
    "        for i, name in enumerate(gpus):\n",
    "            print(f\"  GPU {i} : {name}\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"Erreur lors de l'appel à nvidia-smi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef92bfcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "sklearn needs to be installed in order to use this module",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 3) Load pre-match features and model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m global_df, surface_dfs \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mget_latest_features_by_surface(PARQUET_PATH, CUTOFF_DATE)\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 4) Match-by-match evaluation\u001b[39;00m\n\u001b[1;32m     17\u001b[0m total_evaluated \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/cursos/curso363/TFM/Data-Analytics-with-HPC/Code/0.Utils/utils.py:117\u001b[0m, in \u001b[0;36mload_trained_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_trained_model\u001b[39m(path: Path):\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    Load a trained XGBoost model from the specified path.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_model(path)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/cursos/curso363/mypython/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/cursos/curso363/mypython/lib/python3.10/site-packages/xgboost/sklearn.py:1584\u001b[0m, in \u001b[0;36mXGBClassifier.__init__\u001b[0;34m(self, objective, **kwargs)\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[38;5;129m@_deprecate_positional_args\u001b[39m\n\u001b[1;32m   1578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1579\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1584\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/cursos/curso363/mypython/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/netapp2/Store_uni/home/ulc/cursos/curso363/mypython/lib/python3.10/site-packages/xgboost/sklearn.py:772\u001b[0m, in \u001b[0;36mXGBModel.__init__\u001b[0;34m(self, max_depth, max_leaves, max_bin, grow_policy, learning_rate, n_estimators, verbosity, objective, booster, tree_method, n_jobs, gamma, min_child_weight, max_delta_step, subsample, sampling_method, colsample_bytree, colsample_bylevel, colsample_bynode, reg_alpha, reg_lambda, scale_pos_weight, base_score, random_state, missing, num_parallel_tree, monotone_constraints, interaction_constraints, importance_type, device, validate_parameters, enable_categorical, feature_types, feature_weights, max_cat_to_onehot, max_cat_threshold, multi_strategy, eval_metric, early_stopping_rounds, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;129m@_deprecate_positional_args\u001b[39m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    770\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SKLEARN_INSTALLED:\n\u001b[0;32m--> 772\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    773\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn needs to be installed in order to use this module\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators \u001b[38;5;241m=\u001b[39m n_estimators\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m objective\n",
      "\u001b[0;31mImportError\u001b[0m: sklearn needs to be installed in order to use this module"
     ]
    }
   ],
   "source": [
    "# 1) Configuration\n",
    "JSON_PATH    = Path('./Datasets/aus_open_2025_matches_all_ids.json')\n",
    "PARQUET_PATH = Path('./Datasets/final_tennis_dataset_symmetric.parquet')\n",
    "MODEL_PATH   = Path('./Models/xgb_model.json')\n",
    "CUTOFF_DATE  = '2025-01-01'\n",
    "\n",
    "# 2) Load tournament structure\n",
    "with open(JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "    tournament = json.load(f)\n",
    "surface = tournament['surface']\n",
    "\n",
    "# 3) Load pre-match features and model\n",
    "global_df, surface_dfs = utils.get_latest_features_by_surface(PARQUET_PATH, CUTOFF_DATE)\n",
    "model = utils.load_trained_model(MODEL_PATH)\n",
    "\n",
    "# 4) Match-by-match evaluation\n",
    "total_evaluated = 0\n",
    "total_correct   = 0\n",
    "accuracy_by_round = {}\n",
    "\n",
    "for match in tournament['matches']:\n",
    "    m_id    = match['match_id']\n",
    "    rnd     = match['round']\n",
    "    p1_id   = match['player1']['id']\n",
    "    p2_id   = match['player2']['id']\n",
    "    p1_name = match['player1']['name']\n",
    "    p2_name = match['player2']['name']\n",
    "    actual  = match['outcome']\n",
    "\n",
    "    # a) Skip if IDs or real outcome are missing\n",
    "    if p1_id is None or p2_id is None:\n",
    "        print(f\"Skipping {m_id} ({rnd}): missing ID — {p1_name}={p1_id}, {p2_name}={p2_id}\")\n",
    "        continue\n",
    "    if actual is None:\n",
    "        print(f\"Skipping {m_id} ({rnd}): actual outcome missing\")\n",
    "        continue\n",
    "\n",
    "    # b) Try to build features and predict\n",
    "    try:\n",
    "        prob = utils.predict_match(p1_id, p2_id, surface, model, global_df, surface_dfs)\n",
    "    except KeyError as e:\n",
    "        print(f\"Skipping {m_id} ({rnd}): missing features — {e}\")\n",
    "        continue\n",
    "\n",
    "    pred = 'player1' if prob >= 0.5 else 'player2'\n",
    "    is_correct = (pred == actual)\n",
    "\n",
    "    # c) Record the result\n",
    "    total_evaluated += 1\n",
    "    total_correct   += is_correct\n",
    "    stats = accuracy_by_round.setdefault(rnd, {'total': 0, 'correct': 0})\n",
    "    stats['total']   += 1\n",
    "    stats['correct'] += is_correct\n",
    "\n",
    "# 5) Reporting\n",
    "if total_evaluated:\n",
    "    overall_acc = total_correct / total_evaluated\n",
    "    print(f\"\\nEvaluated {total_evaluated} matches; overall accuracy: {overall_acc:.2%}\\n\")\n",
    "else:\n",
    "    print(\"\\nNo matches could be evaluated.\\n\")\n",
    "\n",
    "print(\"Accuracy by round:\")\n",
    "for rnd, stats in accuracy_by_round.items():\n",
    "    acc = stats['correct'] / stats['total']\n",
    "    print(f\"  {rnd}: {acc:.2%} ({stats['correct']}/{stats['total']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35bd47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Semifinals ===\n",
      " match_id     player1    player2  prob_p1    pred  actual  correct\n",
      "      115  Shelton B.  Sinner J. 0.287103 player2 player2        1\n",
      "      116 Djokovic N. Shelton B. 0.528279 player1 player2        0\n",
      "\n",
      "=== The Final ===\n",
      " match_id   player1   player2  prob_p1    pred  actual  correct\n",
      "      117 Sinner J. Zverev A. 0.762118 player1 player1        1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rounds to display\n",
    "detail_rounds = ['Semifinals', 'The Final']\n",
    "\n",
    "# Pour chaque tour, on filtre et on affiche\n",
    "for rnd in detail_rounds:\n",
    "    df_r = df_results[df_results['round'] == rnd]\n",
    "    if df_r.empty:\n",
    "        print(f\"No matches evaluated for {rnd}\\n\")\n",
    "        continue\n",
    "\n",
    "    print(f\"=== {rnd} ===\")\n",
    "    print(df_r[['match_id','player1','player2','prob_p1','pred','actual','correct']]\n",
    "          .to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06de5408",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd92825",
   "metadata": {},
   "source": [
    "## Montecarlo simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6640e5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 1: Finalists = Alcaraz C. vs Norrie C., Winner = Alcaraz C. (win prob 73.71%)\n",
      "Simulation 2: Finalists = Zverev A. vs Diallo G., Winner = Zverev A. (win prob 60.29%)\n",
      "Simulation 3: Finalists = Sinner J. vs Shelton B., Winner = Shelton B. (win prob 28.97%)\n",
      "Simulation 4: Finalists = Michelsen A. vs Cobolli F., Winner = Cobolli F. (win prob 46.49%)\n",
      "Simulation 5: Finalists = Zverev A. vs Shelton B., Winner = Zverev A. (win prob 49.21%)\n",
      "Simulation 6: Finalists = Sinner J. vs Etcheverry T., Winner = Etcheverry T. (win prob 20.20%)\n",
      "Simulation 7: Finalists = Alcaraz C. vs Krueger M., Winner = Krueger M. (win prob 19.85%)\n",
      "Simulation 8: Finalists = Mensik J. vs Popyrin A., Winner = Popyrin A. (win prob 43.77%)\n",
      "Simulation 9: Finalists = Mensik J. vs Bublik A., Winner = Bublik A. (win prob 33.99%)\n",
      "Simulation 10: Finalists = Fils A. vs Rublev A., Winner = Rublev A. (win prob 51.82%)\n",
      "Simulation 11: Finalists = Kokkinakis T. vs Carreno Busta P., Winner = Kokkinakis T. (win prob 56.35%)\n",
      "Simulation 12: Finalists = Opelka R. vs Van De Zandschulp B., Winner = Van De Zandschulp B. (win prob 42.13%)\n",
      "Simulation 13: Finalists = Opelka R. vs Rune H., Winner = Opelka R. (win prob 47.20%)\n",
      "Simulation 14: Finalists = Alcaraz C. vs Paul T., Winner = Alcaraz C. (win prob 77.35%)\n",
      "Simulation 15: Finalists = Korda S. vs Rublev A., Winner = Rublev A. (win prob 52.68%)\n",
      "Simulation 16: Finalists = Ruud C. vs Bergs Z., Winner = Ruud C. (win prob 64.10%)\n",
      "Simulation 17: Finalists = Stricker D. vs Musetti L., Winner = Stricker D. (win prob 41.92%)\n",
      "Simulation 18: Finalists = Fils A. vs De Minaur A., Winner = De Minaur A. (win prob 53.96%)\n",
      "Simulation 19: Finalists = Seyboth Wild T. vs Rune H., Winner = Seyboth Wild T. (win prob 33.59%)\n",
      "Simulation 20: Finalists = Alcaraz C. vs De Minaur A., Winner = Alcaraz C. (win prob 68.71%)\n",
      "Simulation 21: Finalists = Ruud C. vs De Minaur A., Winner = De Minaur A. (win prob 55.67%)\n",
      "Simulation 22: Finalists = Korda S. vs Cobolli F., Winner = Cobolli F. (win prob 42.98%)\n",
      "Simulation 23: Finalists = Korda S. vs De Minaur A., Winner = De Minaur A. (win prob 50.29%)\n",
      "Simulation 24: Finalists = Korda S. vs Rublev A., Winner = Korda S. (win prob 47.32%)\n",
      "Simulation 25: Finalists = Sinner J. vs Mpetshi G., Winner = Sinner J. (win prob 72.20%)\n",
      "Simulation 26: Finalists = Mensik J. vs De Minaur A., Winner = De Minaur A. (win prob 49.18%)\n",
      "Simulation 27: Finalists = Sinner J. vs Musetti L., Winner = Sinner J. (win prob 72.84%)\n",
      "Simulation 28: Finalists = Sinner J. vs Griekspoor T., Winner = Sinner J. (win prob 73.91%)\n",
      "Simulation 29: Finalists = Marozsan F. vs Shelton B., Winner = Shelton B. (win prob 66.54%)\n",
      "Simulation 30: Finalists = Ruud C. vs Zhang Zh., Winner = Zhang Zh. (win prob 39.97%)\n",
      "\n",
      "After 30 simulations, estimated champion probabilities:\n",
      "De Minaur A.: 13.33% as champion, average final win prob 52.27%\n",
      "Alcaraz C.: 10.00% as champion, average final win prob 73.25%\n",
      "Sinner J.: 10.00% as champion, average final win prob 72.98%\n",
      "Zverev A.: 6.67% as champion, average final win prob 54.75%\n",
      "Shelton B.: 6.67% as champion, average final win prob 47.75%\n",
      "Cobolli F.: 6.67% as champion, average final win prob 44.73%\n",
      "Rublev A.: 6.67% as champion, average final win prob 52.25%\n",
      "Etcheverry T.: 3.33% as champion, average final win prob 20.20%\n",
      "Krueger M.: 3.33% as champion, average final win prob 19.85%\n",
      "Popyrin A.: 3.33% as champion, average final win prob 43.77%\n",
      "Bublik A.: 3.33% as champion, average final win prob 33.99%\n",
      "Kokkinakis T.: 3.33% as champion, average final win prob 56.35%\n",
      "Van De Zandschulp B.: 3.33% as champion, average final win prob 42.13%\n",
      "Opelka R.: 3.33% as champion, average final win prob 47.20%\n",
      "Ruud C.: 3.33% as champion, average final win prob 64.10%\n",
      "Stricker D.: 3.33% as champion, average final win prob 41.92%\n",
      "Seyboth Wild T.: 3.33% as champion, average final win prob 33.59%\n",
      "Korda S.: 3.33% as champion, average final win prob 47.32%\n",
      "Zhang Zh.: 3.33% as champion, average final win prob 39.97%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from importlib.machinery import SourceFileLoader\n",
    "\n",
    "# 1) Load your utils module\n",
    "utils_file = Path(\"../0.Utils/utils.py\").resolve()\n",
    "utils = SourceFileLoader(\"utils\", str(utils_file)).load_module()\n",
    "\n",
    "# 2) Configuration\n",
    "JSON_PATH    = Path('../../Datasets/aus_open_2025_matches_all_ids.json')\n",
    "PARQUET_PATH = Path('../../Datasets/final_tennis_dataset_symmetric.parquet')\n",
    "MODEL_PATH   = Path('../../Models/xgb_model.json')\n",
    "CUTOFF_DATE  = '2025-01-01'\n",
    "MC_RUNS      = 30  # number of Monte Carlo tournament simulations\n",
    "\n",
    "# 4) Load the tournament draw from JSON\n",
    "with open(JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "    tournament = json.load(f)\n",
    "surface = tournament['surface']\n",
    "\n",
    "# 5) Build an ID-to-name map for readable output\n",
    "id_to_name = {}\n",
    "for match in tournament['matches']:\n",
    "    for side in (\"player1\", \"player2\"):\n",
    "        pid = match[side][\"id\"]\n",
    "        name = match[side][\"name\"]\n",
    "        if pid is not None:\n",
    "            id_to_name[pid] = name\n",
    "\n",
    "# 6) Load pre-match feature snapshots and the trained model\n",
    "global_df, surface_dfs = utils.get_latest_features_by_surface(PARQUET_PATH, CUTOFF_DATE)\n",
    "model = utils.load_trained_model(MODEL_PATH)\n",
    "\n",
    "# 7) Prepare the first-round bracket as a list of (player1_id, player2_id)\n",
    "first_round = sorted(\n",
    "    [m for m in tournament['matches'] if m['round'] == '1st Round'],\n",
    "    key=lambda m: m['match_id']\n",
    ")\n",
    "bracket_init = [(m['player1']['id'], m['player2']['id']) for m in first_round]\n",
    "\n",
    "# 8) Single-tournament simulation, returning champion, finalists, and final win probability\n",
    "def simulate_tournament_once():\n",
    "    pairs = list(bracket_init)  # copy initial bracket\n",
    "    rounds = [\n",
    "        '1st Round','2nd Round','3rd Round','4th Round',\n",
    "        'Quarterfinals','Semifinals','The Final'\n",
    "    ]\n",
    "    # simulate all rounds up to the semifinal\n",
    "    for rnd in rounds[:-1]:  # skip final\n",
    "        winners = []\n",
    "        for p1, p2 in pairs:\n",
    "            if p1 is None:\n",
    "                winners.append(p2); continue\n",
    "            if p2 is None:\n",
    "                winners.append(p1); continue\n",
    "            try:\n",
    "                prob_p1 = utils.predict_match(p1, p2, surface, model, global_df, surface_dfs)\n",
    "                winner = p1 if random.random() < prob_p1 else p2\n",
    "            except KeyError as e:\n",
    "                msg = str(e)\n",
    "                if f\"Player {p1}\" in msg:\n",
    "                    winner = p2\n",
    "                elif f\"Player {p2}\" in msg:\n",
    "                    winner = p1\n",
    "                else:\n",
    "                    winner = p2\n",
    "            winners.append(winner)\n",
    "        # pair winners for next round\n",
    "        pairs = [(winners[i], winners[i+1] if i+1 < len(winners) else None)\n",
    "                 for i in range(0, len(winners), 2)]\n",
    "\n",
    "    # now pairs contains exactly one pair for the Final\n",
    "    p1, p2 = pairs[0]\n",
    "    # record finalists\n",
    "    finalists = (p1, p2)\n",
    "    # determine final win probability and winner\n",
    "    if p1 is None:\n",
    "        final_winner = p2\n",
    "        final_prob = 1.0\n",
    "    elif p2 is None:\n",
    "        final_winner = p1\n",
    "        final_prob = 1.0\n",
    "    else:\n",
    "        prob_p1 = utils.predict_match(p1, p2, surface, model, global_df, surface_dfs)\n",
    "        # use the predicted probability as final_prob for the actual winner\n",
    "        if random.random() < prob_p1:\n",
    "            final_winner = p1\n",
    "            final_prob = prob_p1\n",
    "        else:\n",
    "            final_winner = p2\n",
    "            final_prob = 1 - prob_p1\n",
    "\n",
    "    return final_winner, finalists, final_prob\n",
    "\n",
    "# 9) Run Monte Carlo: track champions and final probabilities\n",
    "champion_counts = {}\n",
    "final_probs = {}  # maps champion_id -> list of their final match win probabilities\n",
    "\n",
    "for i in range(1, MC_RUNS + 1):\n",
    "    champion, (f1, f2), prob = simulate_tournament_once()\n",
    "    champion_counts[champion] = champion_counts.get(champion, 0) + 1\n",
    "    final_probs.setdefault(champion, []).append(prob)\n",
    "    name_champ = id_to_name.get(champion, champion)\n",
    "    name_f1 = id_to_name.get(f1, f1)\n",
    "    name_f2 = id_to_name.get(f2, f2)\n",
    "    print(f\"Simulation {i}: Finalists = {name_f1} vs {name_f2}, \"\n",
    "          f\"Winner = {name_champ} (win prob {prob:.2%})\")\n",
    "\n",
    "# 10) Display aggregated results\n",
    "print(f\"\\nAfter {MC_RUNS} simulations, estimated champion probabilities:\")\n",
    "for pid, count in sorted(champion_counts.items(), key=lambda x: -x[1]):\n",
    "    name = id_to_name.get(pid, pid)\n",
    "    probability = count / MC_RUNS\n",
    "    avg_final_prob = sum(final_probs[pid]) / len(final_probs[pid])\n",
    "    print(f\"{name}: {probability:.2%} as champion, \"\n",
    "          f\"average final win prob {avg_final_prob:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypython",
   "language": "python",
   "name": "mypython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
